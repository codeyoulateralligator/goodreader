#!/usr/bin/env python3
"""
goodreads_ester_mapper.py ğŸ‡ªğŸ‡ªğŸ“š
Build b-32 â€¢ 2025-07-02
"""

from __future__ import annotations
# â”€â”€ stdlib â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import argparse, csv, json, os, pathlib, re, sys, time, unicodedata, urllib.parse
from collections import Counter, defaultdict, deque   
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Tuple
# â”€â”€ third-party â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import requests, folium, html as htm
from bs4 import BeautifulSoup
from geopy.geocoders import Nominatim
from geopy.extra.rate_limiter import RateLimiter
from requests.exceptions import ReadTimeout
import hashlib 
import urllib.parse as _uparse


# â”€â”€â”€ debug helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEBUG = bool(int(os.getenv("ESTER_DEBUG", "0")))
CLR = {k: f"\x1b[{v}m" for k, v in dict(dim=90, cyan=36, yel=33,
                                         grn=32, red=31, pur=35, reset=0).items()}
_WHITES = re.compile(r"\s{2,}")
def log(tag, msg, col="dim", err=False):
    stream = sys.stderr if err or DEBUG else sys.stdout
    print(f"{CLR[col]}{tag}{CLR['reset']} {msg}", file=stream, flush=True)
def dbg(tag, msg="", col="red"):
    if DEBUG:
        log(tag, msg, col)

FAILED: List[str] = []                 # titles with zero *KOHAL*
HTML_CACHE: dict[str, str] = {}
RECORD_URL: dict[tuple[str, str], str] = {}
RECORD_BRIEF: dict[str, str] = {}
RECORD_ISBN: dict[str, str] = {}       # record-url  â†’  isbn13
_BRIEF_CACHE: dict[str, str] = {} 
_ID_SEEN: set[str] = set()
_SURNAME_CLEAN = re.compile(r"[^a-z0-9]+")
_ERS_CACHE: dict[str, bool] = {}   # record-URL â†’ verdict  (memo)

# â”€â”€â”€ constants â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
UA = "goodreads-ester/1.32"
HDRS = {"User-Agent": UA}
TIMEOUT, PAUSE = 30, 1      # sec
ESTER  = "https://www.ester.ee"
SEARCH = f"{ESTER}/search~S8*est"
GOODREADS_SHELF = "https://www.goodreads.com/review/list"

GEOCACHE = pathlib.Path(".geocache.json")
POPUP_CSS = ("font-size:1.5em;"
             "max-width:1000px;"
             "max-height:400px;"
             "overflow:auto;"
             "white-space:nowrap;")

_DASH = re.compile(r"[\u2010-\u2015\u2212]")
_SLUG = re.compile(r"[^A-Z0-9]")

# â”€â”€â”€ static location tables (LIBRARY_META & BRANCH_META) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
LIBRARY_META: Dict[str, Tuple[str, str]] = {
    "RaRa": ("Eesti Rahvusraamatukogu", "TÃµnismÃ¤gi 2, Tallinn, Estonia"),
    "TÃœR": ("Tartu Ãœlikooli Raamatukogu", "W. Struve 1, Tartu, Estonia"),
    "TLÃœAR": ("Tallinna Ãœlikooli Akadeemiline RK", "RÃ¤vala puiestee 10, Tallinn, Estonia"),
    "TalTech": ("TalTech Raamatukogu (peahoone)", "Akadeemia tee 1, Tallinn, Estonia"),
    "EKA": ("Eesti Kunstiakadeemia Raamatukogu", "Kotzebue 1, Tallinn, Estonia"),
    "EMU": ("Eesti MaaÃ¼likooli Raamatukogu", "Fr. R. Kreutzwaldi 1A, Tartu, Estonia"),
    "TlnRK": ("Tallinna Keskraamatukogu (sÃ¼steem)", "Estonia pst 8, Tallinn, Estonia"),
    "Tartu LR": ("Tartu Linnaraamatukogu", "Kompanii 3/5, Tartu, Estonia"),
    "KMAR": ("KaitsevÃ¤e Akadeemia Raamatukogu", "Riia 21, Tartu, Estonia"),
    "KV": ("KaitsevÃ¤e Peastaabi Raamatukogu", "Juhkentali 58, Tallinn, Estonia"),
    "TARTU": ("Tartu Ãœlikooli Raamatukogu", "W. Struve 1, Tartu, Estonia"),
    "TLKR": ("Tallinna Keskraamatukogu â€“ Peahoone", "Estonia pst 8, Tallinn, Estonia"),
}
BRANCH_META = {
    "SÃœDALINNA": ("TKR SÃ¼dalinna", "Estonia pst 8, Tallinn"),
    "LIIVALAIA": ("TKR Liivalaia", "Liivalaia 40, Tallinn"),
    "KADRIORU": ("TKR Kadriorg", "Lydia Koidula 12a, Tallinn"),
    "KALAMAJA": ("TKR Kalamaja", "Kotzebue 9, Tallinn"),
    "KÃ„NNUKUKE": ("TKR KÃ¤nnukuke", "Eduard Vilde tee 72, Tallinn"),
    "LAAGNA": ("TKR Laagna", "VÃµru 11, Tallinn"),
    "MÃ„NNI": ("TKR MÃ¤nni", "Ehitajate tee 48, Tallinn"),
    "MÃ„NNIKU": ("TKR MÃ¤nniku", "Pihlaka 12, Tallinn"),
    "NURMENUKU": ("TKR Nurmenuku", "Ehitajate tee 109a/2, Tallinn"),
    "NÃ•MME": ("TKR NÃµmme", "Raudtee 68, Tallinn"),
    "PAEPEALSE": ("TKR Paepealse", "Paul Pinna 8, Tallinn"),
    "PELGURANNA": ("TKR Pelguranna", "Kangru 13, Tallinn"),
    "PIRITA": ("TKR Pirita", "Metsavahi tee 19, Tallinn"),
    "PÃ„Ã„SKÃœLA": ("TKR PÃ¤Ã¤skÃ¼la", "PÃ¤rnu mnt 480a, Tallinn"),
    "SÃ•LE": ("TKR SÃµle", "SÃµle 47b, Tallinn"),
    "SÃ„Ã„SE": ("TKR SÃ¤Ã¤se", "SÃµpruse pst 186, Tallinn"),
    "TONDI": ("TKR Tondi", "PÃ¤rnu mnt 125, Tallinn"),
    "TORUPILLI": ("TKR Torupilli", "Gonsiori 36, Tallinn"),
    "VÃ„IKEÃ•ISMÃ„E": ("TKR VÃ¤ike-Ã•ismÃ¤e", "Ã•ismÃ¤e tee 115a, Tallinn"),
    "BUSSI": ("TKR Raamatukogubuss", "Tallinn, Estonia"),
}
def _slug(s): return _SLUG.sub("", unicodedata.normalize("NFKD", s)
                                            .encode("ascii","ignore").decode("ascii").upper())
BRANCH_META = {_slug(k): v for k, v in BRANCH_META.items()}

# â”€â”€â”€ misc helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def squeeze(s):    return _WHITES.sub(" ", s)
def norm_dash(s):  return _DASH.sub("-", s)
def strip_parens(t): return re.sub(r"\s*\(.*?\)\s*$", "", t).strip()
def ester_enc(s):  return "".join(ch if ord(ch) < 128 else f"{{u{ord(ch):04X}}}" for ch in s)
def strip_ctrl(t): return "".join(ch for ch in t
                                  if unicodedata.category(ch)[0]!="C"
                                  and unicodedata.category(ch)!="Cf")

def _ascii_fold(s: str) -> str:
    """
    Strip accents/diacritics and return lowercase plain-ASCII.
    """
    return (
        unicodedata
        .normalize("NFKD", s)
        .encode("ascii", "ignore")
        .decode("ascii")
        .lower()
    )

def _surname(author: str) -> str:
    """
    Produce a sortable surname key.

    â€¢ Accepts both  â€œLastname, Firstnameâ€  (Goodreads CSV)
      and           â€œFirstname Lastnameâ€   (occasional fallback).

    â€¢ Returned key is ASCII-folded, lowercase, and stripped of
      punctuation/diacritics, so â€œSaint-ExupÃ©ryâ€ â†’ â€œsaintexuperyâ€.
    """
    a = _ascii_fold(author)
    if "," in a:                                   # standard â€œLast, Firstâ€
        last = a.split(",", 1)[0]
    else:                                          # â€œFirst Last â€¦â€
        parts = _SURNAME_CLEAN.split(a)
        last = parts[-1] if parts else a
    return _SURNAME_CLEAN.sub("", last)            # purge leftovers

# â”€â”€â”€ Goodreads CSV loader â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def gd_csv(path: pathlib.Path, limit: int | None):
    out=[]
    with path.open(encoding="utf-8") as fh:
        for row in csv.DictReader(fh):
            if row.get("Exclusive Shelf","").lower()!="to-read": continue
            a = row["Author"].strip()
            t = row["Title"].strip()
            isbn = (row.get("ISBN13") or row.get("ISBN") or "").strip()
            if isbn.startswith('="') and isbn.endswith('"'): isbn = isbn[2:-1]
            out.append((a,t,isbn))
            if limit and len(out)>=limit: break
    return out

_isbn13_re = re.compile(r"\b\d{13}\b")

def _last_isbn13(s: str) -> str:
    """Return the *last* 13-digit sequence in *s* (or '')."""
    all_hits = _isbn13_re.findall(s)
    return all_hits[-1] if all_hits else ""

# â”€â”€â”€ Goodreads web-shelf scraper (public profile) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def parse_web_shelf(uid: str, limit: int | None):
    """
    Scrape the public â€œtable viewâ€ of a Goodreads userâ€™s To-Read shelf.

    The ISBN column renders as:
        <span class="greyText">13</span>9789916127209
    so we MUST pick the *last* 13-digit substring.
    """
    out, page = [], 1
    while True:
        url = (f"{GOODREADS_SHELF}/{uid}"
               "?shelf=to-read&per_page=200"
               f"&page={page}&sort=date_added&view=table")
        soup = BeautifulSoup(_download(url), "html.parser")

        rows = soup.select("tr[id^='review_']")
        if not rows:
            break

        for r in rows:
            a = r.select_one("td.field.author   a")
            t = r.select_one("td.field.title    a")
            raw = r.select_one("td.field.isbn13")
            # take the LAST 13-digit run, ignore leading â€œ13â€ label
            digits = re.findall(r"\b\d{13}\b", raw.get_text()) if raw else []
            isbn   = digits[-1] if digits else ""
            if a and t:
                out.append((a.get_text(strip=True),
                            t.get_text(strip=True),
                            isbn))
            if limit and len(out) >= limit:
                return out
        page += 1
    return out

# â”€â”€â”€ put this near the other helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _canon_name(token: str) -> str:
    """
    Give a *lossy but stable* ASCII fingerprint for one surname token so that
    the countless spellings of the same Slavic/Baltic name collapse to the
    **same** string.

    Example collapses
    -----------------
        AleksijevitÅ¡  â†’  alexivich
        Alexievits    â†’  alexivich
        Alexievich    â†’  alexivich
        Alexivich     â†’  alexivich   (already canonical)

    Pipeline
    --------
      0. lower-case
      1. digraph fixes that still carry diacritics   (tÅ¡â†’ch, dÅ¾â†’j, â€¦)
      2. ASCII-fold remaining diacritics/umlauts
      3. cheap phonetic conflations                 (ksâ†’x, hhâ†’kh, jâ†’y)
      4. collapse   double letters
      4Â½.   -vits / -ievits / -yevits   â†’   -vich
      5. glide simplifications           yi/iy/ie/ye  â†’  i
      6. drop trailing i/y glides
      7. strip everything that is *not* aâ€“z
    """
    # â”€â”€ 0. lower-case â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    s = token.lower()

    # â”€â”€ 1. digraphs with diacritics (before ASCII-fold!) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    s = (s.replace("tÅ¡", "ch")
           .replace("dÅ¾", "j"))

    # â”€â”€ 2. ASCII-fold: map the few Estonian diacritics we meet â”€â”€â”€â”€â”€â”€â”€
    _TR = str.maketrans({
        "Å¡": "sh", "Å¾": "zh",
        "Ãµ": "o",  "Ã¤": "a",
        "Ã¶": "o",  "Ã¼": "u",
    })
    s = s.translate(_TR)

    # â”€â”€ 3. coarse phonetic conflations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    s = (s.replace("ks", "x")      # Aleks â†’ Alex
           .replace("hh", "kh")
           .replace("j", "y"))     # Ğ˜Ğ²Ğ°Ğ½ â†’ Ivan-y (Ğ¹ â†’ y)

    # â”€â”€ 4. collapse runs of doubled letters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    s = re.sub(r"(.)\1+", r"\1", s)

    # â”€â”€ 4Â½. *-vits / -ievits / -yevits* â†’ *-vich*  (most common case) â”€
    s = re.sub(r"(?:[iey]?v)its$", "vich", s)

    # â”€â”€ 5. glide simplifications  (yi/iy/ie/ye â†’ i) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    #     This erases the e/Ñ‹ difference responsible for
    #     Alexievich   vs  Alexivich    &c.
    s = (s.replace("yi", "i")
           .replace("iy", "i")
           .replace("ie", "i")
           .replace("ye", "i"))

    # â”€â”€ 6. kill any leftover trailing i/y glide â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    s = re.sub(r"[iy]+$", "", s)

    # â”€â”€ 7. allow only plain ASCII letters in the final key â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    s = re.sub(r"[^a-z]", "", s)

    return s


_FRSCRUB = re.compile(r"""
     (?:&\d+(?:,\d+)*,?$)                 # tail like â€œâ€¦&1,1,â€ or â€œâ€¦&0,0,â€
   | (?:[&?](?:save|saved|clear_saves)=[^&]*)   # ?save=â€¦, &saved=â€¦, â€¦
""", re.I | re.X)

# â”€â”€â”€ replace the whole _canon() with the version below â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _canon(url: str) -> str:
    """
    Normalise *hit-list* URLs so that cosmetic junk
    (slice counters, â€œsaveâ€ parameters) cannot generate
    an infinite stream of â€œnewâ€ pages.

    â€¢ If the link is a Sierra hit-list (contains â€œ/framesetâ€ anywhere)
      â€“ keep the meaningful part up to &FF=â€¦
      â€“ strip trailing slice counters (&1,1, â€¦) and the save/saved
        noise ESTER appends each time you click.
    â€¢ Otherwise drop the whole query string â€“ ordinary pages donâ€™t need it
      for uniqueness.
    """
    s = _uparse.urlsplit(url)
    path  = _uparse.unquote(s.path)
    query = _uparse.unquote(s.query)

    if "/frameset" in path or "/frameset" in query:
        tail = f"{path}?{query}" if query else path
        tail = _FRSCRUB.sub("", tail)                # â† NEW: scrub junk
        return _uparse.urlunsplit((s.scheme, s.netloc, tail, "", ""))

    # non-frameset: ignore ?params entirely
    return _uparse.urlunsplit((s.scheme, s.netloc, path, "", ""))

_MAX_VISITED = 60          # safety valve
_BAD_LEADS = (
    "/clientredirect", "/patroninfo~", "/validate/patroninfo",
    "/requestmulti~",  "/mylistsmulti", "/logout",

    # â€œSave recordâ€ noise produced by Sierra/ESTER
    "?save=",         #  â† NEW  (was only â€œ&save=â€)
    "&save=",
    "?saved=",        #  â† NEW
    "&saved=",
    "?clear_saves=",  #  â† NEW
    "&clear_saves=",
    "/frameset&save",     # old variant
    "?save_func=",
)

def collect_record_links(start_url: str) -> list[str]:
    """
    Breadth-first crawl starting at *start_url* and return the **first**
    physical ESTER record link (``.../record=bNNNNNNN~S8*est``) encountered.

    The walk stops as soon as one such link is found or after the safety
    limit of 60 distinct pages has been reached.

    Links whose URL matches anything in the `_BAD_LEADS` blacklist are
    skipped immediately â€“ they are patron-session / redirect boiler-plate
    that can never contain holdings pages and often lead to infinite loops.
    """
    q: deque[str] = deque([start_url])
    seen: set[str] = set()

    while q:
        url = q.popleft()
        key = _canon(url)
        if key in seen:
            continue
        seen.add(key)

        dbg("collect open", url)
        soup = BeautifulSoup(_download(url), "html.parser")

        # â”€â”€ 1. harvest record links already on this page â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        for a in soup.select('a[href*="/record=b"]'):
            rec = _uparse.urljoin(url, a["href"])
            if _is_eresource(rec):
                dbg("collect", f"    skip E-RES {rec[-28:]}")
                continue
            dbg("collect", f"    âœ“ physical {rec[-28:]}")
            return [rec]                        # â† EARLY EXIT

        # â”€â”€ 2. enqueue inner documents (framesets / iframes) â”€â”€â”€â”€â”€â”€â”€â”€â”€
        leads = (
            [f["href"] for f in soup.select('a[href*="/frameset"]')] +
            [f["src"]  for f in soup.select('frame[src], iframe[src]')]
        )

        for l in leads:
            if not l:
                continue
            nxt = _uparse.urljoin(url, l)

            # skip anything on the do-not-touch list
            if any(bad in nxt or nxt.startswith(bad) for bad in _BAD_LEADS):
                dbg("collect", f"    skip bad-lead {nxt[:80]}")
                continue

            if _canon(nxt) in seen:
                continue
            if len(seen) >= _MAX_VISITED:
                dbg("collect", "    abort â€“ visited>60")
                break

            q.append(nxt)
            dbg("collect", f"    add lead {nxt}")

    dbg("collect", "    âˆ… 0 physical copies")
    return []

# â”€â”€â”€ tokenisers / surname helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_norm_re=re.compile(r"[^a-z0-9]+")
def _ascii_fold(s): return unicodedata.normalize("NFKD",s).encode("ascii","ignore").decode("ascii").lower()
def _tokenise(s):   return {tok for tok in _norm_re.split(_ascii_fold(s)) if tok}
def _surname(a):    # supports â€œLastname, Firstnameâ€
    if "," in a: return _ascii_fold(a.split(",",1)[0]).split()[0]
    p=_ascii_fold(a).split(); return p[-1] if p else ""

# â”€â”€â”€ HTTP download (cached) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SESSION = requests.Session()
def _download(url: str) -> str:
    if url in HTML_CACHE:
        return HTML_CACHE[url]
    try:
        r = SESSION.get(url, headers=HDRS, timeout=TIMEOUT, allow_redirects=True)
        r.raise_for_status()
    except requests.HTTPError as e:
        dbg("GET-ERR", f"{url[-60:]} â†’ {e.response.status_code}")
        HTML_CACHE[url] = ""          # remember itâ€™s useless
        return ""                     # let caller treat as â€œno dataâ€
    HTML_CACHE[url] = r.text
    return r.text

# â”€â”€â”€ ESTER utilities â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _ester_fields(rec):
    soup=BeautifulSoup(_download(rec),"html.parser")
    ttl=soup.select_one("h1.title, h2.title") \
        or soup.select_one("td#bibTitle") \
        or soup.select_one("td.bibInfoLabel:-soup-contains('Pealkiri')+td.bibInfoData")
    aut=soup.select_one("td.bibInfoLabel:-soup-contains('Autor')+td.bibInfoData")
    return (strip_ctrl(ttl.get_text(" ",strip=True)) if ttl else "",
            strip_ctrl(aut.get_text(" ",strip=True)) if aut else "")

_ERS_TAGS = (
    "1 vÃµrguressurss", "tekstifail", "audiofile", "videosalvestis",
    "vÃµrguteavik", "1 online resource", "online resource",
    "electronic bk", "electronic resource",
    "e-raamat", "ebook", "e-audiobook", "e-raamat",
    "digitaalkogu", "internetivÃ¤ljaanne", "pdf (online)", "pdf-fail",
    "www-link",
)

def _is_eresource(rec_url: str) -> bool:
    """
    Treat the record as a **virtual-only** item **iff**

      â€“ the record page shows *no* holdings table **and**
      â€“ at least one string from `_ERS_TAGS` occurs in the HTML.

    (Uses `_ERS_CACHE` for memoisation.)
    """
    if rec_url in _ERS_CACHE:             # memoised result â†’ instant
        return _ERS_CACHE[rec_url]

    html = _download(rec_url)             # your existing cached GET
    soup = BeautifulSoup(html, 'html.parser')

    has_physical = bool(
        soup.select_one("tr.bibItemsEntry, tr[class*=bibItemsEntry]")
    )
    if has_physical:
        _ERS_CACHE[rec_url] = False
        dbg("_is_eresource", f"{rec_url} â†’ False (physical holdings present)")
        return False

    page_lc = html.lower()
    eres    = any(tag.lower() in page_lc for tag in _ERS_TAGS)
    _ERS_CACHE[rec_url] = eres
    dbg("_is_eresource", f"{rec_url} â†’ {eres}")
    return eres

# ---------------------------------------------------------------------
# _looks_like_same_book â€“ new body with Slavic-name canonicalisation
# ---------------------------------------------------------------------
def _looks_like_same_book(w_ttl: str, w_aut: str, rec_url: str) -> bool:
    """
    Decide whether the ESTER record behind *rec_url* is the same book as
    (*w_ttl* /*w_aut*).  Returns True for a match.
    """
    # 1. pull title / author from the record page ---------------------
    r_ttl, r_aut = _ester_fields(rec_url)
    if not r_ttl:                        # fetch / parse failed
        return False

    # 2. tokenise -----------------------------------------------------
    wanted_toks = _tokenise(strip_parens(w_ttl))
    record_toks = _tokenise(r_ttl) | _tokenise(r_aut)

    # ---- TITLE test -------------------------------------------------
    ttl_ok = wanted_toks <= record_toks            # need *all* wanted words

    # ---- AUTHOR test (new) ------------------------------------------
    # a) take the surname part(s) of the wanted author
    surname_raw   = _surname(w_aut)                # â€œAleksijevitÅ¡â€ etc.
    surname_parts = _tokenise(surname_raw)         # {'aleksijevits'}

    # b) canonicalise both sides to tolerate AleksijevitÅ¡ â†” Alexievich
    wanted_canon  = {_canon_name(p) for p in surname_parts}
    record_canon  = {_canon_name(t) for t in record_toks}

    auth_ok = not surname_parts or wanted_canon <= record_canon

    # ---- DEBUG dump (unchanged) -------------------------------------
    if DEBUG:
        print("â”Œâ”€  title/author comparator â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        print(f"â”‚  record URL      : {rec_url}")
        print(f"â”‚  wanted ttl      : {w_ttl!r}")
        print(f"â”‚  record ttl      : {r_ttl!r}")
        print(f"â”‚  wanted aut      : {w_aut!r}")
        print(f"â”‚  record aut      : {r_aut!r}")
        print(f"â”‚  wanted toks     : {sorted(wanted_toks)}")
        print(f"â”‚  record toks     : {sorted(record_toks)}")
        print(f"â”‚  surname parts   : {sorted(surname_parts)}")
        print(f"â”‚  canon wanted    : {sorted(wanted_canon)}")
        print(f"â”‚  canon record    : {sorted(record_canon)}")
        verdict = 'MATCH' if ttl_ok and auth_ok else 'SKIP'
        colour  = 'grn' if verdict == 'MATCH' else 'red'
        print(f"â””â”€â”€ verdict: {CLR[colour]}{verdict}{CLR['reset']}\n")

    return ttl_ok and auth_ok


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Helper: pretty-print one (loc, status) pair
def _dbg_pair(tag: str, pair: tuple[str, str]):
    loc, sta = pair
    dbg(tag, f"{loc[:38]:38} {sta}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â¶  Scrape one HTML holdings page
def _scrape_holdings_html(url: str) -> list[tuple[str, str]]:
    dbg("holdings", f"HTMLâ†’ {url}")
    html   = _download(url)
    dbg("holdings", f"      {len(html):,} B fetched")

    soup   = BeautifulSoup(html, "html.parser")
    rows   = soup.select("#tab-copies tr[class*='bibItemsEntry'], "
                         ".additionalCopies tr[class*='bibItemsEntry']")
    out: list[tuple[str, str]] = []
    for r in rows:
        tds = r.find_all("td")
        if len(tds) >= 3:
            out.append((strip_ctrl(tds[0].get_text()).strip(),
                        strip_ctrl(tds[2].get_text()).strip().upper()))

    dbg("holdings", f"      {len(out)} rows matched selector")
    if DEBUG and not out:
        dbg("holdings", "      saving HTML preview")
        with open("debug_empty_holdings.html", "w", encoding="utf-8") as fh:
            fh.write(html[:1200])            # write only first 1 200 B

    N = 2                             # how many rows to dump verbosely
    if DEBUG and out:
        for loc, sta in out[:N]:
            _dbg_pair("holdings", (loc, sta))
        if len(out) > N:
            dbg("holdings", f"... (+{len(out) - N} more)")

    return out

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â·  Ask EPiK JSON endpoint
def _copies_via_api(bib: str) -> list[tuple[str, str]]:
    api = "https://epik.ester.ee/api/data/getItemsByCodeList"
    dbg("holdings", f"APIâ†’ {api}  payload=[{bib!r}]")
    try:
        r = requests.post(api, json=[bib], timeout=10)
        dbg("holdings", f"      HTTP {r.status_code}, {len(r.content):,} B")
        r.raise_for_status()
        data = r.json()[0]            # list with one element
    except Exception as e:
        dbg("holdings", f"API error: {e}")
        return []

    items = data.get("items", [])
    dbg("holdings", f"      {len(items)} item objects")

    out: list[tuple[str, str]] = []
    for it in items:
        loc = it.get("libraryNameEst") or it.get("libraryName") or ""
        sta = (it.get("statusEst") or it.get("status") or "").upper()
        out.append((strip_ctrl(loc), strip_ctrl(sta)))

    if DEBUG and out:
        _dbg_pair("holdings", out[0])
        if len(out) > 1:
            _dbg_pair("holdings", out[1])

    return out

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â¸  Master helper
def holdings(rec: str) -> list[tuple[str, str]]:
    """
    â‘  fetch â€¦/holdings~   â†’ parse
    â‘¡ fetch â€¦/holdingsa~  â†’ parse
    â‘¢ EPiK JSON API       â†’ parse
    returns [(location, STATUS), â€¦]
    """
    m = re.search(r"\b(b\d{7})", rec)
    if not m:
        dbg("holdings", "cannot find bib-id in record URL")
        return []
    bib = m.group(1)

    # -- step â‘  : classic ---------------------------------------------
    base = (f"{ESTER}/search~S8*est?/.{bib}/.{bib}/1,1,1,B/holdings~{bib}"
            "&FF=&1,0,/indexsort=-")
    rows = _scrape_holdings_html(base)
    if rows:
        return rows

    # -- step â‘¡ : â€œavailable copiesâ€ page ------------------------------
    alt  = base.replace("holdings~", "holdingsa~")
    rows = _scrape_holdings_html(alt)
    if rows:
        return rows

    # -- step â‘¢ : EPiK JSON -------------------------------------------
    dbg("holdings", "HTML empty â€“ switching to EPiK API")
    return _copies_via_api(bib)

def resolve(loc):
    if loc.startswith("TlnRK"):
        rest=loc.removeprefix("TlnRK").lstrip(" ,:-")
        key=_slug(rest.split()[0]) if rest else ""
        return BRANCH_META.get(key,("Tallinna Keskraamatukogu","Tallinn")) if key else LIBRARY_META["TLKR"]
    for sig,(n,a) in LIBRARY_META.items():
        if loc.startswith(sig): return n,a
    return loc,""

# â”€â”€â”€ colour helper (this was the missing bit) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def pcol(n:int)->str:
    if n==1: return "red"
    if n<=3: return "orange"
    if n<=7: return "beige"
    return "green"

def _safe_id(raw: str) -> str:
    """
    Convert *raw* into a stable, HTML- and CSS-safe id attribute.
    - keep ASCII letters, numbers, '-' and '_'
    - collapse other runs of chars into a single '-'
    - guarantee uniqueness by appending a counter if needed
    """
    # 1. slugify
    slug = re.sub(r'[^0-9A-Za-z_-]+', '-', raw).strip('-_')
    if not slug:                                           # pathological case
        slug = 'id-' + hashlib.md5(raw.encode()).hexdigest()[:8]

    # 2. ensure uniqueness
    if slug in _ID_SEEN:
        base, n = slug, 2
        while f"{base}-{n}" in _ID_SEEN:
            n += 1
        slug = f"{base}-{n}"
    _ID_SEEN.add(slug)
    return slug

# ----------------------------------------------------------------------
# Low-level helper â€“ talks to the hidden â€œavalancheâ€ REST endpoint
# ----------------------------------------------------------------------
def _avalanche_cover(recnum: str, session: requests.Session | None = None) -> str:
    """
    Ask the avalanche micro-service for cover art of one ESTER record.

    Parameters
    ----------
    recnum   ESTER record number, e.g. "b2404042".
    session  optional requests.Session for connection reuse.

    Returns
    -------
    The best image URL (or a data:-URI) or "" if not available.
    """
    api = "https://epik.ester.ee/ester/avalanche?ids"
    sess = session or requests

    try:
        resp = sess.post(api, json=[recnum], timeout=10)
        resp.raise_for_status()
        meta = resp.json()[0]           # we asked for one record â†’ list[0]
        print(meta)
        if meta.get("imageData"):       # base-64 payload
            return f"data:image/jpeg;base64,{meta['imageData']}"

        return meta.get("urlLarge") or meta.get("urlSmall") or ""
    except Exception:
        # network problems, JSON errors, etc. â€“ pretend nothing was found
        return ""
    
_BAD_IMG_PAT = re.compile(r'(?i)(/screens/|spinner|transparent\.gif|\.svg$)')
_GOOGLE      = "https://books.google.com/books/content?vid=ISBN{0}&printsec=frontcover&img=1&zoom=1"
_OPENLIB     = "https://covers.openlibrary.org/b/isbn/{0}-M.jpg"
IMG_ATTRS    = ("data-src", "data-original", "data-large", "data-iiif-high", "src")

def _looks_like_jacket(src: str) -> bool:
    return (
        src
        and (src.startswith(("http://", "https://", "/iiif/")) or src.startswith("data:image/"))
        and not _BAD_IMG_PAT.search(src)
    )

def _abs(src: str) -> str:
    return src if src.startswith(("http", "data:")) else f"{ESTER}{src}"

def _scrape_isbns(soup: BeautifulSoup) -> list[str]:
    isbns = []
    for a in soup.select('a[href*="isbn"]'):
        m = re.search(r'\b\d{9}[\dXx]|\d{13}\b', a.get_text())
        if m:
            isbns.append(m.group(0))
    return isbns

def _first_good_url(urls: list[str]) -> str:
    for u in urls:
        dbg(f"Trying URL: {u}")
        try:
            # Cloudinary: HEAD â†’ 404, but GET (no body) â†’ 302 â†’ 200
            r = requests.get(u, stream=True, timeout=5, allow_redirects=True)
            ct = r.headers.get("Content-Type", "")
            dbg(f"GET {u} â†’ {r.status_code} ({ct})")
            if r.ok and ct.startswith("image"):
                dbg(f"Selected image URL: {r.url}")
                return r.url                       # r.url == final location
        except Exception as e:
            dbg(f"Exception for URL {u}: {e}")
    dbg("No good image URL found.")
    return ""

def _via_search_thumb(code: str) -> str:
    """Return the cover thumb that appears only in the search-results page."""
    url = f"https://www.ester.ee/search~S8*est/X?searchtype=X&searcharg={code}"
    print(f"search-thumb URL: {url}")

    try:
        soup = BeautifulSoup(requests.get(url, timeout=8).text, "html.parser")

        img = soup.select_one("td.briefcitPhotos img[src]")
        if not img:
            print("search-thumb: no <img> found")
            return ""

        src = img["src"]
        if not src.startswith(("http://", "https://")):
            src = f"https://www.ester.ee{src}"     # make absolute

        print(f"search-thumb candidate: {src}")
        return src if _first_good_url([src]) else ""
    except Exception as e:
        print(f"search-thumb error: {e}")
        return ""
# ---------------------------------------------------------------------------

def _find_cover(soup: BeautifulSoup) -> str:
    # STEP 1 â”€ look at whatâ€™s already in the HTML ---------------------------
    for tag in soup.select("img, noscript img"):
        for attr in IMG_ATTRS:
            if (src := tag.get(attr) or "").strip() and _looks_like_jacket(src):
                return _abs(src)

    og = soup.find("meta", property="og:image") or soup.find("meta", attrs={"name": "og:image"})
    if og and _looks_like_jacket(og.get("content", "")):
        return _abs(og["content"])

    # STEP 2 â”€ EPiK API (+ fallbacks) --------------------------------------
    html = str(soup)
    m = (
        re.search(r'record=([b\d]+)', html)
        or re.search(r'catalog/([b\d]+)', html)
        or re.search(r'"code"\s*:\s*"([b\d]+)"', html)
    )
    if not m:
        return ""

    code = m.group(1)
    api  = "https://epik.ester.ee/api/data/getImagesByCodeList"
    try:
        item = requests.post(api, json=[code], timeout=10).json()[0]
    except Exception:
        item = None

    # 2a â€“ inline base-64
    if item and item.get("imageData"):
        return "data:image/jpeg;base64," + item["imageData"]

    # 2b â€“ IIIF (try record-code *before* uuid)
    iiif_code = f"https://www.ester.ee/iiif/2/{code}/full/500,/0/default.jpg"
    if _first_good_url([iiif_code]):
        return iiif_code      # âœ… works for list thumbs and often for full cover

    # 2c â€“ IIIF with UUID (rarely used nowadays)
    uuid = item and item.get("idUuid")
    if uuid:
        iiif_uuid = f"https://epik.ester.ee/iiif/2/{uuid}/full/500,/0/default.jpg"
        if _first_good_url([iiif_uuid]):
            return iiif_uuid

    # 2c â€“ Google Books / Open Library by ISBN
    isbn_urls = [
        _GOOGLE.format(i) for i in _scrape_isbns(soup)
    ] + [
        _OPENLIB.format(i) for i in _scrape_isbns(soup)
    ]
    return _first_good_url(isbn_urls)


def _record_brief(rec, fallback_title: str = "", isbn: str = "") -> str:
    """
    Build a one-liner HTML snippet for the map pop-up.

    `rec` may be:
        â€¢ BeautifulSoup Tag / Soup
        â€¢ raw HTML string
        â€¢ record URL (str)
    """
    # --- 1. Retrieve / parse -------------------------------------------------
    soup: BeautifulSoup | None = None
    url: str | None = None

    if hasattr(rec, "select_one"):          # Tag / Soup
        soup = rec if isinstance(rec, BeautifulSoup) else BeautifulSoup(str(rec), "html.parser")

    elif isinstance(rec, str):
        if rec.startswith("http"):          # a link â€“ maybe cached already
            url = rec
            if url in _BRIEF_CACHE:
                return _BRIEF_CACHE[url]

            html = _download(url)
            soup = BeautifulSoup(html, "html.parser")
        else:                               # raw HTML
            soup = BeautifulSoup(rec, "html.parser")

    # --- 2. Extract bits -----------------------------------------------------
    title = author = ""

    if soup:
        ttl_el = (
            soup.select_one(".bibInfoData strong")
            or soup.select_one("h1.title, h2.title, td#bibTitle")
        )
        aut_el = (
            soup.select_one(".bibInfoData a[href*='/author']")
            or soup.select_one("td.bibInfoLabel:-soup-contains('Autor')+td.bibInfoData")
        )

        title  = ttl_el.get_text(strip=True) if ttl_el else ""
        author = aut_el.get_text(strip=True) if aut_el else ""

    # graceful fall-backs
    if not title:
        # fallback_title is something like "Author â€“ Title" â†’ keep only title part
        if " â€“ " in fallback_title:
            title = fallback_title.split(" â€“ ", 1)[1]
        else:
            title = fallback_title or "â€”"

    # --- 3. Cover hunt -------------------------------------------------------
    cover = _find_cover(soup) if soup else ""
    if not cover and isbn:
        cover = _openlib_link(isbn, "M")    # OpenLibrary, if it exists

    cover_html = f'<img src="{cover}" style="height:120px;float:right;margin-left:.6em;">' if cover else ""

    # â”€â”€ 4. Compose & cache â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    link_start = f'<a href="{htm.escape(url)}" target="_blank" rel="noopener noreferrer">' if url else ""
    link_end   = '</a>' if url else ""

    if author:
        text  = f"{htm.escape(author)} â€“ <em>{htm.escape(title)}</em>"
    else:
        text  = f"<em>{htm.escape(title)}</em>"

    brief = f"{cover_html}{link_start}{text}{link_end}"

    if url:
        _BRIEF_CACHE[url] = brief
    return brief

_JS_SNIPPET = r"""
<script>
/* Helper: rebuild the heading line on demand ------------------------ */
function headingHTML(hasSel){
  return (hasSel
            ? '<button id="closeSel" onclick="clearSelection()" '
              + 'style="float:right;font-weight:bold;background:#eee;'
              + 'border:1px solid #999;border-radius:3px;width:1.6em;'
              + 'height:1.6em;line-height:1.2em;cursor:pointer;">&times;</button>'
            : '')
       + '<h3 style="margin:0;">Valitud raamatud</h3>';
}

/* Simple HTML-entity unescape for tooltips -------------------------- */
function decodeHTML(str){
  const t = document.createElement('textarea');
  t.innerHTML = str;
  return t.value;
}

const chosen = new Map();

/* Re-render the panel ------------------------------------------------ */
function refreshPanel(){
  const box     = document.getElementById("selectionBox");
  const hasSel  = chosen.size > 0;
  const items   = Array.from(chosen.values())
                       .map(txt => "<p>"+txt+"</p>")
                       .join("<hr style='margin:.4em 0;width:100%;'>");
  box.innerHTML = headingHTML(hasSel) + items;
}

/* Clear everything --------------------------------------------------- */
function clearSelection(){
  for (const id of chosen.keys()){
    const li = document.getElementById(id);
    if (li) li.style.fontWeight = "normal";
  }
  chosen.clear();
  refreshPanel();
}

/* Toggle one book in/out of the list --------------------------------- */
function toggleBook(id){
  const li = document.getElementById(id);
  if (chosen.has(id)){
    chosen.delete(id);
    li.style.fontWeight = "normal";
  } else {
    chosen.set(id, decodeHTML(li.dataset.brief));
    li.style.fontWeight = "bold";
  }
  refreshPanel();
}
</script>

<style>
#selectionBox{
  position:fixed; top:1rem; right:1rem; z-index:9999;
  max-width:340px; max-height:90vh; overflow:auto;
  background:#fff; border:2px solid #444; padding:0.5rem 1rem;
  box-shadow:0 0 8px rgba(0,0,0,.3); font-size:0.9em;
}
</style>

<!-- initial empty panel -->
<div id="selectionBox">
  <!-- headingHTML(false) -->
  <h3 style="margin:0;">Valitud raamatud</h3>
</div>
"""


# â”€â”€â”€ search helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _probe(label: str, url: str) -> list[str]:
    """
    Run one ESTER probe, log hit-count *and* the probe-URL, then return
    the list of record links found.
    """
    links = collect_record_links(url)
    hits  = len(links)

    colour = "grn" if hits else "red"
    log("ğŸ›° probe", f"{label:<11} {hits} hit(s)", colour)
    log("â†³", url, "dim")                     # always print the URL

    return links

def by_isbn(isbn): 
    return _probe("keyword-isbn",f"{SEARCH}/X?searchtype=X&searcharg={isbn}"
                                               "&searchscope=8&SORT=DZ&extended=0&SUBMIT=OTSI")
def by_title_index(t):
    q=ester_enc(norm_dash(t))
    return _probe("title-idx",f"{SEARCH}/X?searchtype=t&searcharg="
                              f"{urllib.parse.quote_plus(q,safe='{}')}"
                              "&searchscope=8&SORT=DZ&extended=0&SUBMIT=OTSI")
def by_keyword(a,t):
    raw=squeeze(f"{a} {t}"); q=ester_enc(norm_dash(raw))
    return _probe("keyword-ttl",f"{SEARCH}/X?searchtype=X&searcharg="
                                f"{urllib.parse.quote_plus(q,safe='{}')}"
                                "&searchscope=8&SORT=DZ&extended=0&SUBMIT=OTSI")
def search(author: str, title: str, isbn: str) -> list[str]:
    """
    Return **at most one** ESTER record URL â€“ the first one that fits.

    Strategy
    --------
      1. ISBN (unique)             â†’ accept immediately if a physical record is found
      2. title-index search        â†’ require title/author token match
      3. keyword  â€œauthor titleâ€   â†’ ditto
      4. keyword  â€œtitleâ€          â†’ ditto
    """
    # -- normalise ----------------------------------------------------
    title = strip_parens(title)

    # -- â‘  ISBN probe -------------------------------------------------
    if isbn:
        links = by_isbn(isbn)
        if links:
            # The catalogue confirmed the ISBN and the record is physical
            return links[:1]           # short-circuit â€“ no extra checks

    # -- â‘¡â€“â‘£ fallback probes -----------------------------------------
    probes = (
        (by_title_index, title),
        (by_keyword,      (author, title)),
        (by_keyword,      ("", title)),
    )

    for fn, arg in probes:
        if not arg:                             # empty title etc.
            continue

        links = fn(*arg) if isinstance(arg, tuple) else fn(arg)

        for rec in links:                       # validate on the fly
            if _looks_like_same_book(title, author, rec):
                return [rec]                    # first convincing hit

    return []                                   # nothing matched


# â”€â”€â”€ worker â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _openlib_link(isbn13: str, size: str = "M") -> str:
    # size = S | M | L   â€“ whatever you prefer
    return (
        f"https://covers.openlibrary.org/b/isbn/{isbn13}-{size}.jpg"
        "?default=false"          # << *** key change  ***
    )   

# â”€â”€â”€ worker â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def process_title(idx: int, total: int,
                  author: str, title: str, isbn: str) -> tuple[Counter, dict]:
    """
    â€¢ log progress
    â€¢ call `search()`
    â€¢ collect *KOHAL* holdings for the (single) matching record
    """
    t0 = time.time()
    log(f"[{idx:3}/{total}]", f"{author} â€“ {title}", "cyan")
    log("ğŸ”– ISBN:", isbn or "â€” none â€”", "pur")

    copies: Counter                   = Counter()
    meta:   dict[str, tuple[str, str]] = {}

    recs = search(author, title, isbn)
    if not recs:                                      # zero matches
        log("âœ—", "no matching record on ESTER", "red")
        FAILED.append(f"{author} â€“ {title}" + (f" (ISBN {isbn})"
                                               if isbn else ""))
        log("â³", f"{time.time() - t0:.2f}s", "pur")
        return copies, meta

    rec = recs[0]                                     # exactly one
    RECORD_URL[(author, title)] = rec
    RECORD_ISBN[rec]            = isbn or ""
    _record_brief(rec, f"{author} â€“ {title}", isbn or "")

    # â”€â”€ holdings ------------------------------------------------------------
    for loc, status in holdings(rec):
        name, addr = resolve(loc)
        key        = f"{name}|{addr}"

        if "KOHAL" in status:
            copies[(author, title, key)] += 1

        meta[key] = (name, addr)

    tot = sum(copies.values())
    log("âœ“" if tot else "âœ—",
        f"{tot} Ã— KOHAL" if tot else "0 Ã— KOHAL",
        "grn" if tot else "red")

    if not tot:
        FAILED.append(f"{author} â€“ {title}"
                      + (f" (ISBN {isbn})" if isbn else ""))

    log("â³", f"{time.time() - t0:.2f}s", "pur")
    return copies, meta

# â”€â”€â”€ map builder â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_map(lib_books, meta, coords, outfile):
    """
    lib_books { key â†’ [(display, url)  OR  display_string, â€¦] }
    meta      { key â†’ (pretty_name, address) }
    coords    { key â†’ (lat, lon) }
    """
    if not coords:
        log("!", "Nothing available (KOHAL) on ESTER", "yel", err=True)
        return

    # â”€â”€ centre the map roughly on the mean location â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    lats = [la for la, _ in coords.values()]
    lons = [lo for _, lo in coords.values()]
    centre = (sum(lats) / len(lats), sum(lons) / len(lons))

    m = folium.Map(location=centre, zoom_start=11)
    folium.Element(
        "<style>.leaflet-popup-content{max-width:1600px;}</style>"
    ).add_to(m)

    # include the JS/CSS for the â€œselectionâ€ side-panel
    m.get_root().html.add_child(folium.Element(_JS_SNIPPET))

    # â”€â”€ iterate every library that has copies â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    for key, entries in lib_books.items():
        if key not in coords:
            continue
        lat, lon = coords[key]
        lib_name, _addr = meta[key]

        # â”€â”€ NEW: sort list items by author surname (case-/accent-less) â”€â”€
        def _sort_key(entry):
            disp = entry[0] if isinstance(entry, (tuple, list)) else entry
            author_part = disp.split("â€“", 1)[0].strip()      # â€œLastname, Firstnameâ€
            return _surname(author_part)

        entries_sorted = sorted(entries, key=_sort_key)

        # â”€â”€ build the <li> elements (unchanged except we use entries_sorted) â”€â”€
        lis = []
        for entry in entries_sorted:
            disp, url = (
                (entry[0], entry[1]) if isinstance(entry, (tuple, list))
                else (entry, "")
            )
            elem_id = _safe_id(lib_name + disp)
            brief_html = (
                _record_brief(url, disp, RECORD_ISBN.get(url, ""))
                if url else htm.escape(disp)
            )
            brief_attr = (
                brief_html.replace("&", "&amp;")
                          .replace('"', "&quot;")
                          .replace("'", "&#39;")
            )
            lis.append(
                f'<li id="{elem_id}" data-brief="{brief_attr}" '
                f'style="cursor:pointer;color:#06c;" '
                f'onclick="event.stopPropagation();toggleBook(\'{elem_id}\')">'
                f'{htm.escape(disp)}</li>'
            )

        html_popup = (
            f"<div style='{POPUP_CSS}'>"
            f"<b>{htm.escape(lib_name)}</b> "
            f"<span style='color:#666;font-size:90%;'>"
            f"({len(entries_sorted)} pealkirja)</span>"
            f"<ul>{''.join(lis)}</ul></div>"
        )

        folium.Marker(
            location=[lat, lon],
            popup=folium.Popup(html_popup, max_width=1600, min_width=300),
            icon=folium.Icon(color=pcol(len(entries_sorted)),
                             icon="book", prefix="fa"),
        ).add_to(m)

    # â”€â”€ write file â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    m.save(outfile)
    log("âœ“", f"[Done] {outfile}", "grn")

# â”€â”€â”€ main entry-point â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    global DEBUG
    ap=argparse.ArgumentParser()
    gx=ap.add_mutually_exclusive_group(required=True)
    gx.add_argument("--goodreads-csv",type=pathlib.Path)
    gx.add_argument("--goodreads-user")
    ap.add_argument("--max-titles",type=int)
    ap.add_argument("--geocode",action="store_true")
    ap.add_argument("--debug",action="store_true")
    ap.add_argument("--threads",type=int,default=1)
    ap.add_argument("--output",type=pathlib.Path,default="want_to_read_map.html")
    a=ap.parse_args(); DEBUG|=a.debug

    t0 = time.time()

    titles=gd_csv(a.goodreads_csv,a.max_titles) if a.goodreads_csv \
          else parse_web_shelf(a.goodreads_user,a.max_titles)
    log("â„¹",f"{len(titles)} titles","cyan")

    copies=Counter(); meta={}
    if a.threads==1:
        for i,(au,ti,isbn) in enumerate(titles,1):
            c,m=process_title(i,len(titles),au,ti,isbn); copies.update(c); meta.update(m)
    else:
        with ThreadPoolExecutor(max_workers=max(1,a.threads)) as p:
            fut=[p.submit(process_title,i,len(titles),au,ti,isbn)
                 for i,(au,ti,isbn) in enumerate(titles,1)]
            for f in as_completed(fut):
                c,m=f.result(); copies.update(c); meta.update(m)

    if not copies:
        log("!", "Nothing available (KOHAL) on ESTER","red",err=True); return

    lib_books = defaultdict(list)          # key â†’ [(display,url), â€¦]
    for (au, ti, key), n in copies.items():
        display = f"{au} â€“ {ti}" + (f" ({n}Ã—)" if n > 1 else "")
        url     = RECORD_URL.get((au, ti), "")     # may be empty
        lib_books[key].append((display, url))

    geocoder=RateLimiter(Nominatim(user_agent=UA).geocode,min_delay_seconds=1)
    cache=json.loads(GEOCACHE.read_text()) if GEOCACHE.exists() else {}
    coords={}
    for k,(name,addr) in meta.items():
        if not addr: continue
        if not a.geocode and k in cache: coords[k]=cache[k]; continue
        loc=geocoder(addr); time.sleep(1)
        if loc: coords[k]=(loc.latitude,loc.longitude); cache[k]=coords[k]
    if coords: GEOCACHE.write_text(json.dumps(cache,indent=2,ensure_ascii=False))
    build_map(lib_books,meta,coords,a.output)

    if FAILED:
        log("\n=== TITLES WITH NO *KOHAL* COPIES ===","","red")
        for line in FAILED: log("âœ—",line,"red")
        log(f"Total missing: {len(FAILED)}","","red")

    total_time = time.time() - t0
    log("â±", f"Total time spent: {total_time:.2f}s", "yel")

if __name__=="__main__":
    main()
