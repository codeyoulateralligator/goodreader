#!/usr/bin/env python3
"""
goodreads_ester_mapper.py üá™üá™üìö
Build b-26 ‚Ä¢ 2025-06-29
"""

from __future__ import annotations

# ‚îÄ‚îÄ stdlib ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
import argparse
import csv
import json
import os
import pathlib
import re
import sys
import time
import unicodedata
import urllib.parse
import xml.etree.ElementTree as ET
from collections import Counter, defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Tuple

# ‚îÄ‚îÄ third-party ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
import requests
from bs4 import BeautifulSoup
from geopy.extra.rate_limiter import RateLimiter
from geopy.geocoders import Nominatim
from requests.exceptions import ReadTimeout
import folium
import html as htm

# ‚îÄ‚îÄ‚îÄ debug helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
DEBUG = bool(int(os.getenv("ESTER_DEBUG", "0")))
CLR = {k: f"\x1b[{v}m" for k, v in dict(dim=90, cyan=36,
                                         yel=33, grn=32, red=31,
                                         pur=35, reset=0).items()}
_WHITES = re.compile(r"\s{2,}")
FAILED: List[str] = []

def log(tag, msg, col="dim", err=False):
    stream = sys.stderr if err or DEBUG else sys.stdout
    print(f"{CLR[col]}{tag}{CLR['reset']} {msg}", file=stream, flush=True)

def dbg(msg):       # verbose trace when $ESTER_DEBUG=1
    if DEBUG:
        log("‚Ä¢", msg, "red")

try:
    if hasattr(sys.stdout, "reconfigure"):
        sys.stdout.reconfigure(encoding="utf-8", errors="replace")
        sys.stderr.reconfigure(encoding="utf-8", errors="replace")
except Exception:
    pass

# ‚îÄ‚îÄ‚îÄ constants ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
UA              = "goodreads-ester/1.26"
HDRS            = {"User-Agent": UA}
TIMEOUT, PAUSE  = 30, 1

GOODREADS_SHELF = "https://www.goodreads.com/review/list"
ESTER           = "https://www.ester.ee"
SEARCH          = f"{ESTER}/search~S8*est"

GEOCACHE  = pathlib.Path(".geocache.json")
CITY_BOX  = 0.3
CITY_ZOOM = 12
# ‚îÄ‚îÄ at the top of the file ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
POPUP_CSS = (
    "font-size:1em;"
    "max-width:1000px;"
    "max-height:400px;"
    "overflow:auto;"
    "white-space:nowrap;"
)


_DASH = re.compile(r"[\u2010-\u2015\u2212]")
_SLUG = re.compile(r"[^A-Z0-9]")

# ‚îÄ‚îÄ‚îÄ static metadata (unchanged) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
LIBRARY_META = {
    # ‚Äî national & university level ‚Äî
    "RaRa":    ("Eesti Rahvusraamatukogu",              "T√µnism√§gi 2, Tallinn, Estonia"),
    "T√úR":     ("Tartu √úlikooli Raamatukogu",           "W. Struve 1, Tartu, Estonia"),
    "TL√úAR":   ("Tallinna √úlikooli Akadeemiline RK",    "R√§vala puiestee 10, Tallinn, Estonia"),  # you already had this
    "TalTech": ("TalTech Raamatukogu (peahoone)",       "Akadeemia tee 1, Tallinn, Estonia"),
    "EKA":     ("Eesti Kunstiakadeemia Raamatukogu",     "Kotzebue 1, Tallinn, Estonia"),
    "EMU":     ("Eesti Maa√ºlikooli Raamatukogu",        "Fr. R. Kreutzwaldi 1A, Tartu, Estonia"),

    # ‚Äî public-library systems ‚Äî
    "TlnRK":   ("Tallinna Keskraamatukogu (s√ºsteem)",   "Estonia pst 8, Tallinn, Estonia"),
    "Tartu LR":("Tartu Linnaraamatukogu",               "Kompanii 3/5, Tartu, Estonia"),

    # ‚Äî defence / government ‚Äî
    "KMAR":    ("Kaitsev√§e Akadeemia Raamatukogu",      "Riia 21, Tartu, Estonia"),
    "KV":      ("Kaitsev√§e Peastaabi Raamatukogu",      "Juhkentali 58, Tallinn, Estonia"),

    # ‚Äî smaller but recurring ‚Äî
    "TARTU":   ("Tartu √úlikooli Raamatukogu (alias)",   "W. Struve 1, Tartu, Estonia"),
    "TLKR":    ("Tallinna Keskraamatukogu ‚Äì Peahoone",  "Estonia pst 8, Tallinn, Estonia"),      # already in your list
}

BRANCH_META = {
    "S√úDALINNA":   ("TKR S√ºdalinna",        "Estonia pst 8, Tallinn"),
    "LIIVALAIA":   ("TKR Liivalaia",        "Liivalaia 40, Tallinn"),
    "KADRIORU":    ("TKR Kadriorg",         "Lydia Koidula 12a, Tallinn"),
    "KALAMAJA":    ("TKR Kalamaja",         "Kotzebue 9, Tallinn"),
    "K√ÑNNUKUKE":   ("TKR K√§nnukuke",        "Eduard Vilde tee 72, Tallinn"),
    "LAAGNA":      ("TKR Laagna",           "V√µru 11, Tallinn"),
    "M√ÑNNI":       ("TKR M√§nni",            "Ehitajate tee 48, Tallinn"),
    "M√ÑNNIKU":     ("TKR M√§nniku",          "Pihlaka 12, Tallinn"),
    "NURMENUKU":   ("TKR Nurmenuku",        "Ehitajate tee 109a/2, Tallinn"),
    "N√ïMME":       ("TKR N√µmme",            "Raudtee 68, Tallinn"),
    "PAEPEALSE":   ("TKR Paepealse",        "Paul Pinna 8, Tallinn"),
    "PELGURANNA":  ("TKR Pelguranna",       "Kangru 13, Tallinn"),
    "PIRITA":      ("TKR Pirita",           "Metsavahi tee 19, Tallinn"),
    "P√Ñ√ÑSK√úLA":    ("TKR P√§√§sk√ºla",         "P√§rnu mnt 480a, Tallinn"),
    "S√ïLE":        ("TKR S√µle",             "S√µle 47b, Tallinn"),
    "S√Ñ√ÑSE":       ("TKR S√§√§se",            "S√µpruse pst 186, Tallinn"),
    "TONDI":       ("TKR Tondi",            "P√§rnu mnt 125, Tallinn"),
    "TORUPILLI":   ("TKR Torupilli",        "Gonsiori 36, Tallinn"),
    "V√ÑIKE√ïISM√ÑE": ("TKR V√§ike-√ïism√§e",     "√ïism√§e tee 115a, Tallinn"),
    "BUSSI":       ("TKR Raamatukogubuss",  "Tallinn, Estonia"),
}

def slugify(s: str) -> str:
    return _SLUG.sub("", unicodedata.normalize("NFKD", s)
                     .encode("ascii", "ignore").decode("ascii").upper())

BRANCH_META = {slugify(k): v for k, v in BRANCH_META.items()}

# ‚îÄ‚îÄ‚îÄ helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def norm_dash(s: str) -> str: return _DASH.sub("-", s)
def squeeze(s: str) -> str:   return _WHITES.sub(" ", s)
def strip_parens(t: str) -> str: return re.sub(r"\s*\(.*?\)\s*$", "", t).strip()

def ester_enc(s: str) -> str:
    return "".join(ch if ord(ch) < 128 else f"{{u{ord(ch):04X}}}" for ch in s)

def _unwrap_frameset(url: str) -> str:
    """
    If *url* itself is a frameset, fetch it once and return the first
    real /record=b‚Ä¶ link found inside.  Otherwise return *url* unchanged.
    """
    if "frameset" not in url:
        return url                       # already a record page
    soup = BeautifulSoup(_download(url), "html.parser")
    rec = soup.select_one("a[href*='/record=b']")
    return urllib.parse.urljoin(url, rec["href"]) if rec else url

def resolve(loc: str) -> tuple[str, str]:
    """
    Map an ESTER holdings location to (nice-name, address).

    ‚Ä¢ ‚ÄúTlnRK *branch* ‚Ä¶‚Äù  ‚Üí use the *first* word after TlnRK
      (‚ÄúPirita‚Äù, ‚ÄúKalamaja‚Äù, ‚Ä¶) and look it up in BRANCH_META.
    ‚Ä¢ other known SIGL prefixes (RaRa, T√úR ‚Ä¶) via LIBRARY_META.
    ‚Ä¢ everything unknown gets its own marker.
    """
    # ‚Äî‚Äî Tallinna Keskraamatukogu system ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
    if loc.startswith("TlnRK"):
        # strip the prefix and grab just the first word
        rest = loc.removeprefix("TlnRK").lstrip(" ,:-")
        branch_key = slugify(rest.split()[0]) if rest else ""
        if not branch_key:                          # plain ‚ÄúTlnRK‚Äù
            return LIBRARY_META["TLKR"]             # Peahoone
        return BRANCH_META.get(branch_key,
                               ("Tallinna Keskraamatukogu", "Tallinn"))

    # ‚Äî‚Äî all other libraries by their SIGL ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
    for sigl, (name, addr) in LIBRARY_META.items():
        if loc.startswith(sigl):
            return name, addr

    # ‚Äî‚Äî totally unknown string ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
    return loc, ""

def _dbg_resolve(raw: str, out: tuple[str, str]) -> None:
    if DEBUG:
        log("DBG-loc", f"{raw!r} ‚Üí {out!r}", "dim")

def strip_ctrl(t: str) -> str:
    return "".join(ch for ch in t
                   if unicodedata.category(ch)[0] != "C"
                   and unicodedata.category(ch)   != "Cf")

# ‚îÄ‚îÄ‚îÄ helpers used for fuzzy comparisons ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
_norm_re = re.compile(r"[^a-z0-9]+")

def _ascii_fold(s: str) -> str:
    """Unicode ‚Üí pure ASCII, lower-case."""
    return unicodedata.normalize("NFKD", s).encode("ascii", "ignore") \
                        .decode("ascii").lower()

def _tokenise(s: str) -> set[str]:
    """Return a *set* of alphanumeric tokens (ASCII-folded, lower-case)."""
    return {tok for tok in _norm_re.split(_ascii_fold(s)) if tok}

def _surname(author: str) -> str:
    parts = re.sub(r"[^A-Za-z0-9 ]+", " ",
                   unicodedata.normalize("NFKD", author)
                   .encode("ascii", "ignore").decode("ascii")).lower().split()
    return parts[-1] if parts else ""

def _ester_fields(record_url: str) -> tuple[str, str]:
    """
    Return (title, author_text) extracted from one ESTER record page.
      ‚Ä¢ *title*   ‚Üí  the main display title  (Pealkiri  ‚ô¶  √úhtluspealkiri)
      ‚Ä¢ *author*  ‚Üí  the full author field  (Autor ‚Ä¶)

    If parsing fails, either field is returned as "".
    """
    html = _download(record_url)
    soup = BeautifulSoup(html, "html.parser")

    # ‚ù∂ MAIN TITLE  (either <h1 class="title"> or the Pealkiri row)
    tag = soup.select_one("h1.title, h2.title")
    if not tag:                                             # fallback
        tag = soup.select_one("td.bibInfoLabel:-soup-contains('Pealkiri') "
                              "+ td.bibInfoData")
    title = strip_ctrl(tag.get_text(" ", strip=True)) if tag else ""

    # ‚ù∑ AUTHOR
    tag = soup.select_one("td.bibInfoLabel:-soup-contains('Autor') "
                          "+ td.bibInfoData")
    author = strip_ctrl(tag.get_text(" ", strip=True)) if tag else ""

    return title, author

# ‚îÄ‚îÄ‚îÄ quick ‚Äúis this the same book?‚Äù guard  (DEBUG prints kept) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# ‚îÄ‚îÄ‚îÄ quick ‚Äúis this the same book?‚Äù guard  (DEBUG prints kept) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _looks_like_same_book(want_title: str,
                          want_author: str,
                          record_url: str) -> bool:
    """
    True  ‚áí record_url is probably the same book
    False ‚áí skip it

    ‚Ä¢ author‚Äôs **surname** has to appear somewhere in the record
    ‚Ä¢ ‚â• 50 % of wanted-title tokens must overlap with the record title
    """
    # 1) ignore ‚Äú( ‚Ä¶ )‚Äù trailers such as series info or part numbers
    want_title = strip_parens(want_title)

    # 2) pull both title *and* author line from the ESTER record page
    rec_title, rec_author = _ester_fields(record_url)

    # bail if we could not read anything useful
    if not rec_title:
        print(f"{want_title!r} ‚Äî could not extract title ‚Üí skip")
        return False

    # 3) tokenise: ASCII-fold, lower-case, strip punctuation
    rec_toks   = _tokenise(rec_title) | _tokenise(rec_author)
    want_toks  = _tokenise(want_title)
    surname    = _surname(want_author)

    # 4) author surname must be present
    if surname and surname not in rec_toks:
        print(f"{want_title!r} vs {rec_title!r} ‚Üí skip "
              f"(surname {surname!r} not found)")
        return False

    # 5) at least half the title tokens must overlap
    overlap = len(want_toks & rec_toks)
    ok = overlap >= max(1, len(want_toks) // 2)

    print(f"{want_title!r} vs {rec_title!r} ‚Üí "
          f"{'match' if ok else 'skip'} (overlap {overlap}/{len(want_toks)})")
    return ok


# ‚îÄ‚îÄ‚îÄ HTTP ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
SESSION = requests.Session()
def _download(url: str) -> str:
    dbg(f"GET {url}")
    try:
        r = SESSION.get(url, headers=HDRS, timeout=TIMEOUT)
    except ReadTimeout:
        r = SESSION.get(url, headers=HDRS, timeout=TIMEOUT)
    r.raise_for_status()
    return r.text

def _is_eresource(rec_url: str) -> bool:
    """
    Quick test: download the record page once and look for the words that
    ESTER prints for non-physical items.  A page that contains either of
      ‚Ä¢ '[V√µrguteavik]'   (Estonian for ‚Äúe-resource‚Äù)
      ‚Ä¢ 'E-ressursid'     (holding location for e-files)
    is treated as an e-resource.
    """
    page = _download(rec_url).lower()
    return ("v√µrguteavik" in page) or ("e-ressursid" in page)

# ‚îÄ‚îÄ‚îÄ tiny helper: grab ESTER‚Äôs own <h1>/<h2 class="title"> ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _ester_title(url: str,
                 *, _seen: set[str] | None = None,
                 _depth: int = 0,
                 _max_depth: int = 4) -> str:
    """
    Return the display title for one `/record=b‚Ä¶` page.

    Works with:
      ‚Ä¢ the new single-page UI  (has <h1|h2 class="title">)
      ‚Ä¢ the old MARC view       (title sits in <td id="bibTitle">  *or*
                                 next to label ‚ÄúPealkiri‚Äù)
      ‚Ä¢ the legacy frameset UI  (title buried in the *second* frame)
    Falls back to the URL if nothing useful can be extracted.
    """
    if _seen is None:
        _seen = set()
    if url in _seen or _depth >= _max_depth:
        return url               # prevent loops

    _seen.add(url)
    try:
        html = _download(url)
    except Exception:
        return url

    soup = BeautifulSoup(html, "html.parser")

    # ‚ù∂ modern UI ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    tag = soup.select_one("h1.title, h2.title")
    if tag and tag.get_text(strip=True):
        return strip_ctrl(tag.get_text(strip=True))

    # ‚ù∑ old MARC view ‚îÄ
    #    a) direct <td id="bibTitle">
    tag = soup.select_one("td#bibTitle")
    if tag and tag.get_text(strip=True):
        return strip_ctrl(tag.get_text(strip=True))

    #    b) generic two-column layout  (‚ÄúPealkiri‚Äù / title)
    for row in soup.select("tr"):
        lbl = row.select_one("td.bibInfoLabel")
        dat = row.select_one("td.bibInfoData")
        if lbl and dat and "pealkiri" in lbl.get_text(strip=True).lower():
            txt = dat.get_text(strip=True)
            if txt:
                return strip_ctrl(txt)

    # ‚ù∏ frameset ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    for fr in soup.find_all(["frame", "iframe"], src=True):
        child = urllib.parse.urljoin(url, fr["src"])
        got = _ester_title(child,
                           _seen=_seen,
                           _depth=_depth + 1,
                           _max_depth=_max_depth)
        if not got.startswith(("http://", "https://")):  # success
            return got

    # ‚ùπ hopeless ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    return url

def _first_candidate_link(html: str, base: str) -> str | None:
    soup = BeautifulSoup(html, "html.parser")

    # 1) plain record links already in the page
    tag = soup.select_one("a[href*='/record=b']")
    if tag:
        return urllib.parse.urljoin(base, tag["href"])

    # 2) search-result pages: first <h2 class=title> points at a frameset
    tag = soup.select_one("h2.title > a[href*='frameset']")
    if tag:
        return urllib.parse.urljoin(base, tag["href"])

    return None

def collect_record_links(url: str) -> list[str]:
    """
    Return **one** record URL:

    ‚Ä¢ skip the very first hit *if* it is an e-resource;
    ‚Ä¢ otherwise just take it and stop ‚Äì no deep crawling, no loops.
    """
    html = _download(url)
    cand = _first_candidate_link(html, url)
    if cand is None:                       # no hits at all
        return []

    # unwrap the frameset ‚Üí real /record=b‚Ä¶ link
    if "frameset" in cand:
        inner_html = _download(cand)
        real = _first_candidate_link(inner_html, cand)
        cand = real or cand                # fall back if something‚Äôs odd

    # if the chosen record is an e-resource, try the next one **once**
    if _is_eresource(cand):
        soup = BeautifulSoup(html, "html.parser")
        nxt = soup.select("a[href*='/record=b']")
        if len(nxt) > 1:
            cand = urllib.parse.urljoin(url, nxt[1]["href"])

    return [cand]

# ‚îÄ‚îÄ‚îÄ 2. probe wrapper  (just logging + calling the finder) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _probe(label: str, url: str) -> list[str]:
    links = collect_record_links(url)
    hits  = len(links)
    colour = "grn" if hits else "red"
    log("üõ∞ probe", f"{label:<11} {hits} hit(s)", colour)
    if not hits:
        log("‚ãØ", url, "dim")
    return links


# ‚îÄ‚îÄ‚îÄ 3. per-title worker  ‚Äì use the first link *with* KOHAL ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def process_title(idx: int, total: int, author: str, title: str, isbn: str):
    t0 = time.time()
    log(f"[{idx:3}/{total}]", f"{author} ‚Äì {title}", "cyan")
    log("üîñ ISBN:", isbn if isbn else "‚Äî none ‚Äî", "pur")

    local_copies: Counter = Counter()
    local_meta: Dict[str, Tuple[str, str]] = {}

    # --- grab ONLY the top-ranked record ---------------------------------
    candidates = search(author, title, isbn)
    first = candidates[0] if candidates else None

    if first and _looks_like_same_book(title, author, first):
        kohals = holdings(first)          # ‚Üê fetch once
        log("üìñ ESTER", first, "dim")

        if kohals:                        # some copies available
            for loc in kohals:
                name, addr = resolve(loc)
                key = f"{name}|{addr}"
                local_copies[(author, title, key)] += 1
                local_meta[key] = (name, addr)
    # ---------------------------------------------------------------------

    total_kohal = sum(local_copies.values())
    if total_kohal:
        log("‚úì", f"{total_kohal} √ó KOHAL", "grn")
    else:
        log("‚úó", "0 √ó KOHAL", "red")
        FAILED.append(f"{author} ‚Äì {title}"
                      + (f"  (ISBN {isbn})" if isbn else ""))

    log("‚è≥", f"{time.time() - t0:.2f}s", "pur")

    if DEBUG:
        for (a_, t_, key_), n_ in local_copies.items():
            log("DBG-copy", f"{key_:50}  +{n_}", "yel")

    return local_copies, local_meta

# ‚îÄ‚îÄ‚îÄ query builders --------------------------------------------------
def by_isbn(isbn: str) -> list[str]:
    url = (f"{SEARCH}/X?searchtype=X&searcharg={isbn}"
           "&searchscope=8&SORT=DZ&extended=0&SUBMIT=OTSI")
    return _probe("keyword-isbn", url)

def by_title_index(title: str) -> list[str]:
    q   = ester_enc(norm_dash(title))
    url = (f"{SEARCH}/X?searchtype=t&searcharg="
           f"{urllib.parse.quote_plus(q, safe='{}')}"
           "&searchscope=8&SORT=DZ&extended=0&SUBMIT=OTSI")
    return _probe("title-idx", url)

def by_keyword(author: str, title: str) -> list[str]:
    raw  = squeeze(f"{author} {title}".strip())
    q    = ester_enc(norm_dash(raw))
    url  = (f"{SEARCH}/X?searchtype=X&searcharg="
            f"{urllib.parse.quote_plus(q, safe='{}')}"
            "&searchscope=8&SORT=DZ&extended=0&SUBMIT=OTSI")
    return _probe("keyword-ttl", url)

# ‚îÄ‚îÄ‚îÄ dispatcher ------------------------------------------------------
def search(author: str, title: str, isbn: str | None) -> list[str]:
    t_clean = strip_parens(title)
    if isbn:
        if (links := by_isbn(isbn)): return links
    if (links := by_title_index(t_clean)): return links
    if (links := by_keyword(author, t_clean)): return links
    return by_keyword("", t_clean)

# ‚îÄ‚îÄ‚îÄ holdings scraper ------------------------------------------------
def holdings(record_url: str) -> List[str]:
    m = re.search(r"\b(b\d{7})", record_url)
    if not m:
        return []
    bib = m.group(1)
    full = (f"{ESTER}/search~S8*est?/.{bib}/.{bib}/1,1,1,B/"
            f"holdings~{bib}&FF=&1,0,/indexsort=-")
    soup = BeautifulSoup(_download(full), "html.parser")
    rows = soup.select("#tab-copies tr[class*='bibItemsEntry'], "
                       ".additionalCopies tr[class*='bibItemsEntry']")
    out = []
    for r in rows:
        tds = r.find_all("td")
        if len(tds) < 3:
            continue
        if "KOHAL" not in strip_ctrl(tds[2].get_text()).upper():
            continue
        out.append(strip_ctrl(tds[0].get_text()).strip())
    return out

# ‚îÄ‚îÄ‚îÄ Goodreads helpers (unchanged) -----------------------------------
def gd_csv(path: pathlib.Path, limit: int | None):
    out: List[Tuple[str, str, str]] = []
    with path.open(encoding="utf-8") as fh:
        for row in csv.DictReader(fh):
            if row.get("Exclusive Shelf", "").lower() != "to-read":
                continue
            author = row["Author"].strip()
            title  = row["Title"].strip()
            isbn   = (row.get("ISBN13") or row.get("ISBN") or "").strip()
            if isbn.startswith('="') and isbn.endswith('"'):
                isbn = isbn[2:-1]
            out.append((author, title, isbn))
            if limit and len(out) >= limit:
                break
    return out

def gd_api(uid, key, limit):
    out, page = [], 1
    while True:
        r = SESSION.get(GOODREADS_SHELF, headers=HDRS, timeout=TIMEOUT,
                        params=dict(v=2, id=uid, key=key, shelf="to-read",
                                    per_page=200, page=page))
        r.raise_for_status()
        reviews = ET.fromstring(r.content).findall("reviews/review")
        if not reviews:
            break
        for rev in reviews:
            b = rev.find("book")
            out.append((b.findtext("authors/author/name", "").strip(),
                        b.findtext("title_without_series", "").strip(),
                        ""))        # v2 feed has no ISBN
            if limit and len(out) >= limit:
                return out
        page += 1
        time.sleep(PAUSE)
    return out

# ‚îÄ‚îÄ‚îÄ map helpers (unchanged) -----------------------------------------
def gc_load():
    return json.loads(GEOCACHE.read_text()) if GEOCACHE.exists() else {}
def gc_save(c):
    GEOCACHE.write_text(json.dumps(c, indent=2, ensure_ascii=False))

def geocode(key, addr, geo, cache, force):
    if not addr:
        return None

    # already cached
    if not force and key in cache:
        if DEBUG:
            log("DBG-geo", f"cache {key:<45} ‚Üí {cache[key]}", "dim")
        return tuple(cache[key])

    loc = geo(addr)            # Nominatim look-up (rate-limited)
    time.sleep(1)

    if loc:
        cache[key] = (loc.latitude, loc.longitude)
        gc_save(cache)
        if DEBUG:
            log("DBG-geo", f"fresh {key:<45} ‚Üí {(loc.latitude, loc.longitude)}", "grn")
        return loc.latitude, loc.longitude

    log("!", f"geocode FAIL {addr}", "red", err=True)
    return None

def pcol(n: int) -> str:
    if n == 1:
        return "red"
    if n <= 3:
        return "orange"
    if n <= 7:
        return "beige"     # was 'yellow' ‚Üí not accepted by folium.Icon
    return "green"

def build_map(lib_books, meta, coords, outfile):
    """
    lib_books   { key ‚Üí [ "Author ‚Äì Title", ‚Ä¶ ] }
    meta        { key ‚Üí (nice_name, address) }
    coords      { key ‚Üí (lat, lon) }
    outfile     output .html file
    """
    if not coords:
        log("!", "Nothing available (KOHAL) on ESTER", "yel", err=True)
        return

    # centre map on mean lat / lon
    lats = [la for la, _ in coords.values()]
    lons = [lo for _, lo in coords.values()]
    centre = (sum(lats) / len(lats), sum(lons) / len(lons))

    m = folium.Map(location=centre, zoom_start=11)

    # override Leaflet‚Äôs default 300 px CSS limit
    folium.Element(
        "<style>.leaflet-popup-content{max-width:1600px;}</style>"
    ).add_to(m)

    for key, books in lib_books.items():
        if key not in coords:
            continue            # should not happen, but be safe

        lat, lon = coords[key]
        name, _  = meta[key]

        html_popup = (
            f"<div style='{POPUP_CSS}'>"
            f"<b>{htm.escape(name)}</b><ul>"
            + "".join(f"<li>{htm.escape(b)}</li>" for b in books)
            + "</ul></div>"
        )

        folium.Marker(
            location=[lat, lon],
            popup=folium.Popup(
                html_popup,
                max_width=1600,     # ‚Üê widen the bubble
                min_width=300       #   and keep it from shrinking too much
            ),
            icon=folium.Icon(
                color=pcol(len(books)),
                icon="book",
                prefix="fa"
            ),
        ).add_to(m)

    m.save(outfile)
    log("‚úì", f"[Done] {outfile}", "grn")

# ‚îÄ‚îÄ‚îÄ main CLI --------------------------------------------------------
def main():
    global DEBUG
    ap = argparse.ArgumentParser()
    gx = ap.add_mutually_exclusive_group(required=True)
    gx.add_argument("--goodreads-csv", type=pathlib.Path)
    gx.add_argument("--goodreads-user")
    ap.add_argument("--goodreads-key")
    ap.add_argument("--max-titles", type=int)
    ap.add_argument("--geocode", action="store_true")
    ap.add_argument("--debug",   action="store_true")
    ap.add_argument("--threads", type=int, default=1,
                    help="number of worker threads (default 1)")
    ap.add_argument("--output",  type=pathlib.Path,
                    default="want_to_read_map.html")
    args = ap.parse_args()
    if args.debug: DEBUG = True
    if args.goodreads_user and not args.goodreads_key:
        ap.error("--goodreads-key required")

    titles = (gd_csv(args.goodreads_csv, args.max_titles)
              if args.goodreads_csv else
              gd_api(args.goodreads_user, args.goodreads_key, args.max_titles))
    log("‚Ñπ", f"{len(titles)} titles", "cyan")

    copies: Counter = Counter(); meta: Dict[str, Tuple[str, str]] = {}
    if args.threads == 1:
        for idx, (a, t, i) in enumerate(titles, 1):
            c, m = process_title(idx, len(titles), a, t, i)
            copies.update(c); meta.update(m)
    else:
        with ThreadPoolExecutor(max_workers=max(1, args.threads)) as pool:
            fut = [pool.submit(process_title, idx, len(titles), a, t, i13)
                   for idx, (a, t, i13) in enumerate(titles, 1)]
            for f in as_completed(fut):
                c, m = f.result(); copies.update(c); meta.update(m)

    if not copies:
        log("!", "Nothing available (KOHAL) on ESTER", "yel", err=True); return

    lib_books = defaultdict(list)
    for (a, t, key), cnt in copies.items():
        lib_books[key].append(f"{a} ‚Äì {t}" + (f" ({cnt}√ó)" if cnt > 1 else ""))

    geo = RateLimiter(Nominatim(user_agent=UA).geocode, min_delay_seconds=1)
    coords = {k: geocode(k, addr, geo, gc_load(), args.geocode)
              for k, (_, addr) in meta.items() if addr}
    coords = {k: c for k, c in coords.items() if c}
    build_map(lib_books, meta, coords, args.output)

    if FAILED:
        log("\n=== TITLES WITH NO *KOHAL* COPIES ===", "", "red")
        for line in FAILED:
            log("‚úó", line, "red")
        log(f"Total missing: {len(FAILED)}", "", "red")

if __name__ == "__main__":
    main()
